{
 "cells": [
  {
   "cell_type": "raw",
   "id": "vocal-cookie",
   "metadata": {},
   "source": [
    "Basic LLM Practice \n",
    "\n",
    "Includes \n",
    "1) loading pre-trained model \n",
    "2) one-shot learning with pre-trained model \n",
    "3) additional training using custom dataset on top of pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-finnish",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## import packages\n",
    "##################################################\n",
    "!pip install datasets \n",
    "\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## helper function (nicer printing)\n",
    "##################################################\n",
    "\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 80 * '-')\n",
    "    print(textwrap.fill(tokenizer.decode(s, skip_special_tokens=True),80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## instantiating LLM & its tokenizer\n",
    "##################################################\n",
    "\n",
    "model_to_use = \"gpt2\"\n",
    "# model_to_use = \"gpt2-large\"\n",
    "\n",
    "print(\"Using model: \", model_to_use)\n",
    "\n",
    "# get the tokenizer for the pre-trained LM you would like to use\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_to_use)\n",
    "\n",
    "# instantiate a model (causal LM)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_to_use,\n",
    "                                        output_scores=True,\n",
    "                                        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# inspecting the (default) model configuration\n",
    "# (it is possible to created models with different configurations)\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "understood-myrtle",
   "metadata": {},
   "source": [
    "1) Example of zero-shot test generation (decoder-based) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "micro-fancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3260,   262,  1923,   351,   262, 39610,  6842,    11,   314,   357,\n",
      "            64,  6340, 17072,     8,  3066,   284, 15962,  2842,   284, 35278,\n",
      "           680,   616,  1535,   287,   257, 14169,   835,    25,   220]])\n",
      "\n",
      "Top-k sampling:\n",
      "\n",
      "Output:\n",
      "--------------------------------------------------------------------------------\n",
      "After the campaign with the owl bear, I (a paladin) decided to elaborate ways to\n",
      "replenish my health in a clever way:  it seemed like I needed to give myself 10\n",
      "pounds every day or so for 10 days and I was getting sick of taking it or being\n",
      "so sickly, so I got a new one that made me want to eat it.  I would've been\n",
      "tempted to make a more complex plan, but it worked out okay.  When I was eating\n",
      "my favorite foods like kibble and kimchi and being a great person who wasn't\n",
      "addicted to booze I could tell from eating my original\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## autoregressive generation\n",
    "##################################################\n",
    "\n",
    "# text to expand\n",
    "prompt = \"After the campaign with the owl bear, I (a paladin) decided to elaborate ways to replenish my health in a clever way: \"\n",
    "\n",
    "# translate the prompt into tokens\n",
    "input_tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "print(input_tokens)\n",
    "\n",
    "outputs = model.generate(input_tokens,\n",
    "                         max_new_tokens=100,\n",
    "                         do_sample=True,\n",
    "                         top_k=50,\n",
    "                       )\n",
    "\n",
    "print(\"\\nTop-k sampling:\\n\")\n",
    "pretty_print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "competitive-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beam search:\n",
      "\n",
      "Output:\n",
      "--------------------------------------------------------------------------------\n",
      "The barbarian then decided to make a deal with the barbarian. The barbarian\n",
      "agreed to pay the barbarian a certain amount of gold, and the barbarian would\n",
      "give the barbarian the same amount of gold as he would give to the barbarian in\n",
      "exchange for the barbarian's services.  The barbarian was then able to sell the\n",
      "barbarian some of the gold he had received from the barbarian, and he was able\n",
      "to sell it back to him.  After the barbarian had sold the gold\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_tokens,\n",
    "                         max_new_tokens=100,\n",
    "                         num_beams=6,\n",
    "                         no_repeat_ngram_size=4,\n",
    "                         early_stopping=True\n",
    "                         )\n",
    "\n",
    "print(\"\\nBeam search:\\n\")\n",
    "pretty_print(outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "musical-aluminum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Action 1: But then the clever barbarian after encountering the dragon decided to use his power to attack the dragon.\n",
      "\n",
      "The barbarian was able to defeat the dragon by using his magic.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to\n",
      "Alternative Action 2: But then the clever barbarian after encountering the dragon decided to use his power to attack the dragon.\n",
      "\n",
      "The barbarian was able to defeat the dragon by using his magic.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his magic to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to\n",
      "Alternative Action 3: But then the clever barbarian after encountering the dragon decided to use his power to attack the dragon.\n",
      "\n",
      "The barbarian was able to defeat the dragon by using his magic.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian was able to defeat the dragon by using his magic.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his power to attack the dragon.\n",
      "\n",
      "The barbarian then used his\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for generating alternative actions\n",
    "prompt = \"But then the clever barbarian after encountering the dragon decided to\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate alternative actions based on the prompt\n",
    "outputs = model.generate(input_ids,\n",
    "                         max_length=200,\n",
    "                         num_return_sequences=3,  # Specify the number of alternative actions to generate\n",
    "                         do_sample=True,\n",
    "                         top_k=50,\n",
    "                         top_p=0.95,\n",
    "                         temperature=0.7,\n",
    "                         early_stopping=True,\n",
    "                         pad_token_id=tokenizer.eos_token_id,\n",
    "                         num_beams=5,  # You can adjust the number of beams for beam search\n",
    "                         )\n",
    "\n",
    "# Decode and print the generated alternative actions\n",
    "for i, output in enumerate(outputs):\n",
    "    alternative_action = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"Alternative Action {i+1}: {alternative_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "2) One-shot Learning using Pre-trained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "therapeutic-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'output_scores': True}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_chatbot_one_shot/tokenizer_config.json',\n",
       " './fine_tuned_chatbot_one_shot/special_tokens_map.json',\n",
       " './fine_tuned_chatbot_one_shot/vocab.json',\n",
       " './fine_tuned_chatbot_one_shot/merges.txt',\n",
       " './fine_tuned_chatbot_one_shot/added_tokens.json',\n",
       " './fine_tuned_chatbot_one_shot/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the one-shot training data point\n",
    "user_input = \"Hello, how are you?\"\n",
    "expected_response = \"I'm doing well, thank you for asking.\"\n",
    "\n",
    "# Tokenize the data point\n",
    "input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "labels = tokenizer.encode(expected_response, return_tensors=\"pt\")\n",
    "\n",
    "# Ensure input and target sequences have the same length\n",
    "max_length = max(input_ids.size(1), labels.size(1))\n",
    "if input_ids.size(1) != max_length:\n",
    "    input_ids = torch.cat([input_ids, input_ids.new_zeros((input_ids.size(0), max_length - input_ids.size(1)))], dim=-1)\n",
    "if labels.size(1) != max_length:\n",
    "    labels = torch.cat([labels, labels.new_zeros((labels.size(0), max_length - labels.size(1)))], dim=-1)\n",
    "\n",
    "# Fine-tune the model with the one-shot data point\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Adjust the number of epochs as needed\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_chatbot_one_shot\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_chatbot_one_shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "normal-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Hello, how are you?\n",
      "\n",
      "I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm a little bit of a nerd. I'm\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./fine_tuned_chatbot_one_shot\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# User input\n",
    "user_input = \"Hello, how are you?\"\n",
    "\n",
    "# Tokenize the user input\n",
    "input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response using the fine-tuned model\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "\n",
    "# Decode and print the generated response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "3) Pretraining using Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ethical-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.72k/6.72k [00:00<00:00, 19.4MB/s]\n",
      "Downloading data: 100%|██████████| 299M/299M [00:02<00:00, 121MB/s]  \n",
      "Downloading data: 100%|██████████| 23.5M/23.5M [00:00<00:00, 92.5MB/s]\n",
      "Generating train split: 100%|██████████| 650000/650000 [00:03<00:00, 193277.02 examples/s]\n",
      "Generating test split: 100%|██████████| 50000/50000 [00:00<00:00, 165213.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "seeing-romantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "included-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 650000/650000 [04:03<00:00, 2674.50 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:19<00:00, 2564.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "above-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "genetic-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "alternative-mayor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "consolidated-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "affecting-reading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "#         print(f'batch: {batch}')\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-service",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (llm_env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
